有监督回归
    定义：实函数在样本点附近加以近似的有监督的函数近似问题。
    1.最小二乘法
    2.为避免过拟合而设置约束条件的最小二乘法
    3.稀疏学习法
    4.鲁棒学习法
    
-------------------------------------------------------------------------
最小二乘法：Least Squares
    是对模型的输出和训练集输出的平方误差为最小时的参数theta进行学习。

涉及到的概念：
    模型的输出
    训练集的输出
    平方误差
    残差
    范数
    
    设计矩阵
    剑标：广义逆矩阵
    广义逆矩阵的定义与求解方法
-------------------------------------------------------------------------
加权最小二乘学习法

最小二乘解的性质
    奇异值分解  SVDdemo
    正交投影矩阵
    无偏估计
    相关分析：
        知乎：https://www.zhihu.com/question/22237507/answer/53804902
        百科：https://baike.baidu.com/item/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/4968432?fr=aladdin
        奇异值分解（Singular Value Decomposition）是线性代数中一种重要的矩阵分解，
        奇异值分解则是特征分解在任意矩阵上的推广。在信号处理、统计学等领域有重要应用。
-------------------------------------------------------------------------
大规模数据的学习算法：
    随机梯度算法：
        随机梯度法(stochastic gradient method)简称SG法，是一种参数估计算法: SGdemo
        凸函数性质

-------------------------------------------------------------------------
带约束条件的最小二乘法：
    单纯的最小二乘法对于包含噪声的学习过程经常有过拟合的弱点；
    往往是由于学习模型对于训练样本而言过度复杂。
    因此这里学习 可以控制模型复杂程度的---带约束条件的最小二乘法
    1.部分空间约束的最小二乘法学习  Subspace_Constrained_LS
        一般最小二乘法将使用全体参数空间，这里会将参数空间限制在一定范围内，
            来防止过拟合
        正交投影矩阵P通常是手动进行设置的，当然通过使用PCA（主成分分析法）,
            正交投影矩阵也可以基于数据进行设置。


    2.L2约束的最小二乘学习法  L2_Constrained_LS
        部分空间约束的最小二乘学习法中，只使用了部分参数空间，
            但由于正交投影矩阵P的设置有很大自由度，实际操作比较困难。
        L2约束的最小二乘法是以参数空间的原点为圆心，在一定半径范围内的圆
            （一般为超球）内进行求解的。
            利用拉格朗日对偶问题，通过求解最优化问题的解。
            关键字：
                    拉格朗日对偶问题
                    L2约束的最小二乘学习法也称为L2正则化的最小二乘法，在有些著作中也称
                        岭回归。
            -----一般情形------
            圆作为一种特殊的图形，我们可以使用正则化矩阵G，推广为更为一般的表示方法。
            这种更加一般的形式，我们称作：
                一般L2约束的最小二乘学习法
            比如，当G为对称正定矩阵的时候，参数空间被限制在椭圆形状的数据区域内。
            
-------------------------------------------------------------------------
模型选择：
    使用部分空间约束的最小二乘学习法或L2约束的最小二乘学习法，使得最小二乘学习过程中的
    过拟合现象得到了一定程度的缓和，但是这些方法都过于依赖正交投影矩阵P和正则化参数的选择，
    这在一定程序上限制了方法的实际应用，参数选择至关重要。
    另外优化的空间在于：
        若使用线性模型，基函数的种类与数量
        使用核模型，核函数的种类
        L2_Constrained_LS_Compare
        观察结果：带宽h过小，函数呈锯齿状，过大则函数过于平滑
                   正则化参数过小，过拟合就会比较明显，过大则会趋于直线。
        带宽和正则化参数的最优值一般会随着实际函数的种类或噪声的幅度发生变化，因此有必要
        使用不同的输入训练样本，选择合适的带宽和正则化参数。


        像上述这样，不同的输入训练样本决定了机器学习算法中包含的各个参数值，一般称为模型选择。
        作为统计学和机器学习领域的研究热点。  

        模型选择流程：
            1.准备模型的候选。
            2.对各个模型求解其学习结果。
            3.对各学习结果的泛化误差进行评价。
            4.选择泛化误差最小的模型作为最终模型。
