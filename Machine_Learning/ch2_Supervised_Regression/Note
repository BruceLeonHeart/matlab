有监督回归
    定义：实函数在样本点附近加以近似的有监督的函数近似问题。
    1.最小二乘法
    2.为避免过拟合而设置约束条件的最小二乘法
    3.稀疏学习法
    4.鲁棒学习法
    
-------------------------------------------------------------------------
最小二乘法：Least Squares
    是对模型的输出和训练集输出的平方误差为最小时的参数theta进行学习。

涉及到的概念：
    模型的输出
    训练集的输出
    平方误差
    残差
    范数
    
    设计矩阵
    剑标：广义逆矩阵
    广义逆矩阵的定义与求解方法
-------------------------------------------------------------------------
加权最小二乘学习法

最小二乘解的性质
    奇异值分解  SVDdemo
    正交投影矩阵
    无偏估计
    相关分析：
        知乎：https://www.zhihu.com/question/22237507/answer/53804902
        百科：https://baike.baidu.com/item/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/4968432?fr=aladdin
        奇异值分解（Singular Value Decomposition）是线性代数中一种重要的矩阵分解，
        奇异值分解则是特征分解在任意矩阵上的推广。在信号处理、统计学等领域有重要应用。
-------------------------------------------------------------------------
大规模数据的学习算法：
    随机梯度算法：
        随机梯度法(stochastic gradient method)简称SG法，是一种参数估计算法: SGdemo
        凸函数性质

-------------------------------------------------------------------------
带约束条件的最小二乘法：
    单纯的最小二乘法对于包含噪声的学习过程经常有过拟合的弱点；
    往往是由于学习模型对于训练样本而言过度复杂。
    因此这里学习 可以控制模型复杂程度的---带约束条件的最小二乘法
    1.部分空间约束的最小二乘法学习  Subspace_Constrained_LS
        一般最小二乘法将使用全体参数空间，这里会将参数空间限制在一定范围内，
            来防止过拟合
        正交投影矩阵P通常是手动进行设置的，当然通过使用PCA（主成分分析法）,
            正交投影矩阵也可以基于数据进行设置。


    2.L2约束的最小二乘学习法  L2_Constrained_LS
        部分空间约束的最小二乘学习法中，只使用了部分参数空间，
            但由于正交投影矩阵P的设置有很大自由度，实际操作比较困难。
        L2约束的最小二乘法是以参数空间的原点为圆心，在一定半径范围内的圆
            （一般为超球）内进行求解的。
            利用拉格朗日对偶问题，通过求解最优化问题的解。
            关键字：
                    拉格朗日对偶问题
                    L2约束的最小二乘学习法也称为L2正则化的最小二乘法，在有些著作中也称
                        岭回归。
            -----一般情形------
            圆作为一种特殊的图形，我们可以使用正则化矩阵G，推广为更为一般的表示方法。
            这种更加一般的形式，我们称作：
                一般L2约束的最小二乘学习法
            比如，当G为对称正定矩阵的时候，参数空间被限制在椭圆形状的数据区域内。
            
-------------------------------------------------------------------------
模型选择：
    使用部分空间约束的最小二乘学习法或L2约束的最小二乘学习法，使得最小二乘学习过程中的
    过拟合现象得到了一定程度的缓和，但是这些方法都过于依赖正交投影矩阵P和正则化参数的选择，
    这在一定程序上限制了方法的实际应用，参数选择至关重要。
    另外优化的空间在于：
        若使用线性模型，基函数的种类与数量
        使用核模型，核函数的种类
        L2_Constrained_LS_Compare
        观察结果：带宽h过小，函数呈锯齿状，过大则函数过于平滑
                   正则化参数过小，过拟合就会比较明显，过大则会趋于直线。
        带宽和正则化参数的最优值一般会随着实际函数的种类或噪声的幅度发生变化，因此有必要
        使用不同的输入训练样本，选择合适的带宽和正则化参数。


        像上述这样，不同的输入训练样本决定了机器学习算法中包含的各个参数值，一般称为模型选择。
        作为统计学和机器学习领域的研究热点。  

        模型选择流程：
            1.准备模型的候选。
            2.对各个模型求解其学习结果。
            3.对各学习结果的泛化误差进行评价。
            4.选择泛化误差最小的模型作为最终模型。
            
        在做泛化误差评价中，并不会将所有的训练样本用于学习，会留存一部分作为测试样本用来评价
            最后学习结果的泛化误差。-------交叉验证法
            
        
        交叉验证法步骤：
            1.将训练样本随机划分为m个集合（大小要求基本相同）
            2.对i=1,...,m循环执行以下操作：
                a.对除i以外的训练样本，即除开游标当前指向的其余子集进行学习，求解其学习结果。
                b.将游标指向的子集作为测试样本，对a步骤求解的泛化误差进行评估（分为回归和分类）
            3.对每个i的泛化误差进行平均，得到最终的泛化误差的评估值。
            
        说明：
            在分割为m个集合的交叉验证法中，需要进行m次学习，如果m的值很大的话，容易导致计算时间过长。
            然而，对于不同的i而言，学习过程是相互独立的，因此可以使用m台计算机进行并行计算，完美解决以上问题。
            一般将m设定在2到10之间。
            分割数为m的交叉验证法一般称为m折交叉验证，以10折交叉验证最常用。

-------------------------------------------------------------------------
    稀疏学习：
        带约束条件的最小二乘法和交叉验证法的结合，在实际应用是非常有效的回归方法。
        然而，当参数特别多的时候，求解参数和学习得到的函数的输出值的过程，都需要耗费大量的时间。
        稀疏算法定义： 将大部分参数都设置为0的稀疏学习算法，因为大部分参数都变成了0,所以可以快速地求解
            各参数以及学习得到的函数输出值。
            
            L1约束的最小二乘学习法
            在L2约束的最小二乘学习法中，L2范数有一定的约束作用;在稀疏学习中，使用L1进行相应的条件约束。
            有些著作中也称为Lasso回归。
            -------------------------
            求解方法：
                因为L1范数中包含原点除不能微分的绝对值，因此不能像L2约束那样简单地进行求解，但关于稀疏学习
            的最优化研究有较大进展。         
            
